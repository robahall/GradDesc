# GradDesc
Various Gradient Descent algorithms

Current goal is create random data sets and investigate 
how the different gradient descent algorithms speed up the process.

First notebook wrapping my head around plotting the error and the learning path is located in notebooks. 


First Batch:

*   Batch Gradient Descent - update after one epoch (run through all the data once and then update coefficents)
*   Stochastic Gradient Descent (SGD)- update coefficents after random sample group of total batch. 
*   Mini Batch Gradient Descent - update coefficients after small sample of total batch
*   Included visualization of learning rates