# GradDesc
Various Gradient Descent algorithms

Current goal is create random data sets and investigate 
how the different gradient descent algorithms speed up the process.

First Batch:

*   Batch Gradient Descent - update after one epoch (run through all the data once and then update coefficents)
*   Stochastic Gradient Descent (SGD)- update coefficents after random sample group of total batch. 
*   Mini Batch Gradient Descent - update coefficients after small sample of total batch

